# edited by Tianran Zhang at 2016-06-05
#import necessary packages
import numpy as np
import matplotlib as plt
import random
import pandas
from sklearn.feature_selection import SelectKBest
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from mlxtend.classifier import EnsembleVoteClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import cross_validation
from scipy.stats import randint as sp_randint
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# This program contains two parts:
# 1.Data loading:
#   training attributes: x_train, training label: y_train, testing attributes: x_test
# 2.Model training/cross-validation:
#   (1)iteratively perform cross validation on each of the five models
#   (2)calculate mean AUC score.
# 3.Test label prediction
#   (1)choose the model with the highest mean AUC score
#   (2)predict the test labels
#   (3)store results into the predictionsfinal.csv file.
# Note: Please see instructions for evaluating test lable predictions at the end of the code
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
#  Part I: Data loading...
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
print "Loading training data..."
x_train = np.genfromtxt('BinaryTrain_data.csv', delimiter=',')
train_labels_raw = np.genfromtxt('BinaryTrain_sbj_list.csv', delimiter=',', skip_header=1)
y_train = train_labels_raw[:, 1]
print "Loading the given test data..."
x_test = np.genfromtxt('BinaryTest_data.csv', delimiter=',')
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
#  Part II: Model training/cross-validation...
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# First Model: Adaboost
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
all_perf = []
perf = []
print "Training Adaboost..."

for i in range(5):
 AUC_Ada = []

 for train_index, test_index in cross_validation.KFold(n=150, n_folds=5, shuffle=True,
                               random_state=None):
    x_train_cv, x_test_cv = x_train[train_index], x_train[test_index]
    y_train_cv, y_test_cv = y_train[train_index], y_train[test_index]
    bdt_cv = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),
                          n_estimators=300,learning_rate=0.8)
    #print x_train_cv
    bdt_cv.fit(x_train_cv, y_train_cv)
    predictionAda = bdt_cv.predict(x_test_cv)

    AUC_Ada.append(roc_auc_score(y_test_cv, predictionAda))

 perf.append(np.mean(AUC_Ada))

print "======Adaboost Result======"
all_perf.append(0)    # append 0 when don't want to predict using this model
print np.mean(perf)
print "==========================="


# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# Second Model:Bagged Decision Tree
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
perf = []
print "Training Bagged Decision Tree..."
for i in range(5):

 AUC_bag = []

 for train_index, test_index in cross_validation.KFold(n=150, n_folds=5, shuffle=True,
                               random_state=None):
    x_train_cv, x_test_cv = x_train[train_index], x_train[test_index]
    y_train_cv, y_test_cv = y_train[train_index], y_train[test_index]
    bagged_cv = BaggingClassifier(n_estimators=201,
                            max_samples=0.14)
    bagged_cv.fit(x_train_cv, y_train_cv)
    predictionBag = bagged_cv.predict(x_test_cv)
    AUC_bag.append(roc_auc_score(y_test_cv, predictionBag))

 perf.append(np.mean(AUC_bag))

print "===Bagged Decision Tree Result==="
all_perf.append(np.mean(perf))
print np.mean(perf)
print "================================="

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# Third Model: Random Forest
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
perf = []
print "Training Random Forest..."

for i in range(5):

 AUC_rf = []

 for train_index, test_index in cross_validation.KFold(n=150, n_folds=5, shuffle=True,
                               random_state=None):
    x_train_cv, x_test_cv = x_train[train_index], x_train[test_index]
    y_train_cv, y_test_cv = y_train[train_index], y_train[test_index]
    rfc_cv = RandomForestClassifier(n_estimators=300,
                              max_features=40,
                              min_samples_split=2,
                              min_samples_leaf=1)
    rfc_cv.fit(x_train_cv, y_train_cv)
    predictionRF = rfc_cv.predict(x_test_cv)

    AUC_rf.append(roc_auc_score(y_test_cv, predictionRF))

 perf.append(np.mean(AUC_rf))


print "===Random Forest Result==="
all_perf.append(np.mean(perf))
print np.mean(perf)
print "=========================="

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# Fourth Model: Extra Trees
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
perf = []
print "Training Extra Trees Classifier..."

for i in range(5):
 AUC_ET = []

 for train_index, test_index in cross_validation.KFold(n=150, n_folds=5, shuffle=True,
                               random_state=None):
    x_train_cv, x_test_cv = x_train[train_index], x_train[test_index]
    y_train_cv, y_test_cv = y_train[train_index], y_train[test_index]
    ET = ExtraTreesClassifier(n_estimators=400,max_features=20,
                              min_samples_split=2,
                              min_samples_leaf=1)
    ET.fit(x_train_cv, y_train_cv)
    predictionET = ET.predict(x_test_cv)

    AUC_ET.append(roc_auc_score(y_test_cv, predictionET))

 perf.append(np.mean(AUC_ET))


print "===Extra Trees Model Result==="
all_perf.append(np.mean(perf))
print np.mean(perf)
print "=============================="

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# Fifth Model: Meta-ensemble
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
perf = []
print "Training Ensemble Classifier..."

for i in range(5):

 AUC_Ensem=[]
 for train_index, test_index in cross_validation.KFold(n=150, n_folds=5, shuffle=False,
                               random_state=None):
    x_train_cv, x_test_cv = x_train[train_index], x_train[test_index]
    y_train_cv, y_test_cv = y_train[train_index], y_train[test_index]

    bagged1 = BaggingClassifier(n_estimators=201,
                            max_samples=0.14)
    bagged1.fit(x_train_cv, y_train_cv)
    bdt1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),
                          n_estimators=300)
    bdt1.fit(x_train_cv, y_train_cv)
    rfc1 = RandomForestClassifier(n_estimators=400,
                              max_features=40,
                              min_samples_split=2,
                              min_samples_leaf=1)
    rfc1.fit(x_train_cv, y_train_cv)
    ET1 = ExtraTreesClassifier(n_estimators=400,max_features=20,
                              min_samples_split=2,
                              min_samples_leaf=1)

    ET1.fit(x_train_cv, y_train_cv)
    predictionEF = ET1.predict(x_test_cv)
    predictionAda = bdt1.predict(x_test_cv)

    predictionBag = bagged1.predict(x_test_cv)
    predictionRF = rfc1.predict(x_test_cv)
    predictionEnsemble= []
    for i in range (30):

        if predictionAda[i] + predictionBag[i] + predictionRF[i]+predictionEF[i] > 1.9:
            predictionEnsemble.append(1)
        else:
            predictionEnsemble.append(0)

    AUC_Ensem.append(roc_auc_score(y_test_cv, predictionEnsemble))
 perf.append(np.mean(AUC_Ensem))


print "===Ensemble Model Result==="
all_perf.append(np.mean(perf))
print np.mean(perf)
print "==========================="
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
#  Part III: Prediction...
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
print "##########################Prediction...#################################################"
if all_perf.index(max(all_perf)) == 0:
        print "Predicting Adaboost..."
        bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),
                          n_estimators=300,learning_rate=0.8)
        bdt.fit(x_train, y_train)
        predictionAda = bdt.predict(x_test)
        f = open('/Users/zhangtianran/Downloads/MICCAI_2014_MLC-master/predictionsfinal.csv', 'w')
        for i in range(100):
            f.write(str(int(predictionAda[i])) + '\n')
        print "prediction writing finished."
if all_perf.index(max(all_perf)) == 1:
        print "Predicting Bagged..."

        bagged = BaggingClassifier(n_estimators=201,
                            max_samples=0.14)
        bagged.fit(x_train, y_train)
        predictionBag = bagged.predict(x_test)
        f = open('/Users/zhangtianran/Downloads/MICCAI_2014_MLC-master/predictionsfinal.csv', 'w')
        for i in range(100):
            f.write(str(int(predictionBag[i])) + '\n')
        print "prediction writing finished."

if all_perf.index(max(all_perf)) == 2:
        print "Predicting Random Forest..."
        rfc = RandomForestClassifier(n_estimators=300,
                              max_features=40,
                              min_samples_split=2,
                              min_samples_leaf=1)

        rfc.fit(x_train, y_train)
        predictionRF = rfc.predict(x_test)
        f = open('/Users/zhangtianran/Downloads/MICCAI_2014_MLC-master/predictionsfinal.csv', 'w')
        for i in range(100):
            f.write(str(int(predictionRF[i])) + '\n')
        print "prediction writing finished."

if all_perf.index(max(all_perf)) == 3:
        print "Predicting ExtraTrees..."

        ET = ExtraTreesClassifier(n_estimators=400,max_features=20,
                              min_samples_split=2,
                              min_samples_leaf=1)
        ET.fit(x_train, y_train)
        predictionET = ET.predict(x_test)
        f = open('/Users/zhangtianran/Downloads/MICCAI_2014_MLC-master/predictionsfinal.csv', 'w')
        for i in range(100):
            f.write(str(int(predictionET[i])) + '\n')
        print "prediction writing finished."

if all_perf.index(max(all_perf)) == 4:
        print "Predicting Ensemble..."
        bagged1 = BaggingClassifier(n_estimators=201,
                            max_samples=0.14)
        bagged1.fit(x_train, y_train)
        bdt1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),
                          n_estimators=300)
        bdt1.fit(x_train, y_train)
        rfc1 = RandomForestClassifier(n_estimators=400,
                              max_features=40,
                              min_samples_split=2,
                              min_samples_leaf=1)
        rfc1.fit(x_train, y_train)
        ET1 = ExtraTreesClassifier(n_estimators=400,max_features=20,
                              min_samples_split=2,
                              min_samples_leaf=1)

        ET1.fit(x_train, y_train)
        predictionEF = ET1.predict(x_test)
        predictionAda = bdt1.predict(x_test)

        predictionBag = bagged1.predict(x_test)
        predictionRF = rfc1.predict(x_test)
        predictionEnsemble= []
        for i in range (100):

            if predictionAda[i] + predictionBag[i] + predictionRF[i]+predictionEF[i] > 1.9:
                predictionEnsemble.append(1)
            else:
                predictionEnsemble.append(0)
        f = open('/Users/zhangtianran/Downloads/MICCAI_2014_MLC-master/predictionsfinal.csv', 'w')
        for i in range(100):
            f.write(str(int(predictionEnsemble[i])) + '\n')
        print "prediction writing finished."

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# Test Evaluation Instruction:
# (1)Store the test label file in the same folder as the codes
# (2)Uncomment codes below and replace test label file name goes here with test label file name
# (3)Run the code again.
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

#result = np.genfromtxt('predictionsfinal.csv', delimiter=',')
#y_test = np.genfromtxt('test label file name goes here ', delimiter=',')
#print roc_auc_score(y_test,result )
